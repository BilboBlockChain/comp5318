{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 11 - Neural Networks & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2019**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To learn about Perceptron\n",
    "* To learn about dimensionality reduction \n",
    "* To learn about simple DNN 2 layers\n",
    "* To learn about CNN\n",
    "* To learn about Dropout, Pooling\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab11.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab11.ipynb\" file\n",
    "* Complete exercises in \"lab10.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran\n",
    "\n",
    "Tutors: Fengxiang He, Shaojun Zhang, Fangzhou Shi, Yang Lin, Iwan Budiman, Zhiyi Wang, Canh Dinh, Yixuan Zhang, Rui Dong, Haoyu He, Dai Hoang Tran, Peibo Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2)\n",
    "\n",
    "means = [[-1, 0], [1, 0]]\n",
    "cov = [[.3, .2], [.2, .3]]\n",
    "N = 10\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "\n",
    "X = np.concatenate((X0, X1), axis = 0)\n",
    "y = np.concatenate((np.ones(N), -1*np.ones(N)))\n",
    "\n",
    "#fig,axs=plt.subplots(nrows,ncols,figsize=(width,height))\n",
    "plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has 2 classes in total. We will use perceptron algorithm to make a decision boundary for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In perception, we minimizes the number of errors on the training dataset follows:\n",
    "\n",
    "$$ \\epsilon = \\sum_n 1_{[y_n \\neq sign(w^tx_n)]}$$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "For a randomly chosen data point $(x_n, y_n)$ make small changes to $w$ so that : $y_n = sign(w^tx_n)$:\n",
    "\n",
    "2 case:\n",
    "- If $y_n = sign(w^tx_n)$: Do nothing\n",
    "- If $y_n \\neq sign(w^tx_n)$: update w : $w_{t+1} = w_{t} + y_nx_n$ => we only update $w$ for misclassified point. The update process will be stopped when there is no misclassified point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Perception Algorithm from scratch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, X):    \n",
    "    '''\n",
    "    predict label of each row of X, given w \n",
    "    '''\n",
    "    return np.sign(X.dot(w))\n",
    "\n",
    "def perceptron(X, y, w_init):\n",
    "    w = w_init\n",
    "    w_hist = [w]\n",
    "    mis_points = [] # list of misclassified points\n",
    "    while True:\n",
    "        pred = predict(w, X)\n",
    "        mis_idxs = # TODO: Get all misclassified points\n",
    "        num_mis = mis_idxs.shape[0]\n",
    "        if num_mis == 0:\n",
    "            return (w_hist, mis_points)\n",
    "        # random pick one misclassified point \n",
    "        random_id = np.random.choice(mis_idxs, 1)[0]\n",
    "        mis_points.append(random_id)\n",
    "        w = # TODO: Update weight vector\n",
    "        w_hist.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(73)\n",
    "Xbar = np.concatenate((np.ones((2*N, 1)), X), axis = 1)\n",
    "d = Xbar.shape[1]\n",
    "w_init = np.random.randn(d)\n",
    "w_hist, m = perceptron(Xbar, y, w_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the change of boundary during the learning processs depends on number of update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages    \n",
    "def draw_line(plt, w):\n",
    "    w0, w1, w2 = w[0], w[1], w[2]\n",
    "    if w2 != 0:\n",
    "        x11, x12 = -100, 100\n",
    "        return plt.plot([x11, x12], [-(w1*x11 + w0)/w2, -(w1*x12 + w0)/w2], 'k')\n",
    "    else:\n",
    "        x10 = -w0/w1\n",
    "        return plt.plot([x10, x10], [-100, 100], 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = m \n",
    "if(len(m) >= 3):\n",
    "    ncols = 3\n",
    "nrows = math.ceil(len(m)/3)\n",
    "width = 4.5*ncols \n",
    "height = 3.5*nrows\n",
    "\n",
    "filename = 'pla_visualize1.pdf'\n",
    "with PdfPages(filename) as pdf: \n",
    "    plt.close('all')\n",
    "    fig,axs=plt.subplots(nrows,ncols,figsize=(width,height))\n",
    "    ids = range(len(m)+1)\n",
    "    for i, k in enumerate(ids[1:]):\n",
    "        #print(\"i,k\",i,k)\n",
    "        r = i//ncols \n",
    "        c = i%ncols \n",
    "        str0 = 'iter {}/{}'.format(i+1, len(ids)-1)\n",
    "        if(nrows > 1):\n",
    "            temp = axs[r, c]\n",
    "        else:\n",
    "            temp = axs[c]\n",
    "        if nrows > 1:\n",
    "            temp.set_title(str0)\n",
    "            temp.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n",
    "            temp.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n",
    "            temp.plot(0, 0, 'k.', markersize = 8, alpha = .8)\n",
    "            temp.axis([0 , 6, -2, 4])\n",
    "            draw_line(temp, w_hist[k])\n",
    "            wx, wy = w_hist[k][1], w_hist[k][2]\n",
    "            temp.annotate('', xy=(wx, wy), xytext=(0, 0),arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'green', fc = 'green'))\n",
    "        if k < nrows*ncols:\n",
    "            \n",
    "            # get misclassified point\n",
    "            xmis = X[m[k], 0] \n",
    "            ymis = X[m[k], 1]\n",
    "            \n",
    "            #circle around the misclassified point\n",
    "            circle = plt.Circle((xmis, ymis), 0.2, color='k', fill = False)\n",
    "            temp.add_artist(circle)\n",
    "            \n",
    "            #vector to xmis\n",
    "            temp.annotate('', xy=(xmis, ymis), xytext=(0, 0),\n",
    "                        arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'orange', fc = 'orange'))\n",
    "\n",
    "            if m[k] > 10:\n",
    "                #New w if the misclassified point is in red \n",
    "                temp.annotate('', xytext=(0, 0), xy=(wx - xmis, wy-ymis),\n",
    "                        arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'red', fc = 'red'))\n",
    "            else: # the misclassified point is in blue\n",
    "                temp.annotate('', xytext=(0, 0), xy=(wx + xmis, wy+ymis),\n",
    "                        arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'red', fc = 'red'))\n",
    "                \n",
    "        temp.axis('scaled')\n",
    "        temp.axis([-3, 3, -1.5, 2.5])\n",
    "    pdf.savefig(bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue points: Class 1, Red points: Class -1\n",
    "\n",
    "\n",
    "**Green Vector** is $w_t$.\n",
    "\n",
    "**Red Vertor** is $w_{t+1}$\n",
    "\n",
    "The circled points are misclassified point ($x_i$)\n",
    "\n",
    "**Orange Vector** is xi\n",
    "\n",
    "- If $y_i = 1$ (blue), red vector = Sum(Green Vector, Orange Vector)\n",
    "- If $y_i = âˆ’1$, red vector = Sub(Green Vector, Orange Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple autoencoder\n",
    "The goal here is to generate a representation of our data with lower dimension. To do so, autoencoders first transform the data to a low-dimension representation using an encoder network, and then transform the low-dimension representation back to the original space using a decoder network.\n",
    "\n",
    "Here we start with a single fully-connected neural layer as encoder and as decoder. We will be encoding MNIST digit images (dim=784) into a space of dimension 32, hence the compression factor is 784/32 = 24.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we use CNN in classify MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST data from Keras\n",
    "We use one of the dataset included in Keras: MNIST (https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_val = x_test[:5000,:]\n",
    "x_test = x_test[5000:,:]\n",
    "y_val = y_test[:5000]\n",
    "y_test = y_test[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# Reshape data to fit the autoencoder layout and normalize it\n",
    "x_train_ae = x_train.astype('float32') / 255.\n",
    "x_test_ae = x_test.astype('float32') / 255.\n",
    "x_val_ae = x_val.astype('float32') / 255.\n",
    "x_train_ae = x_train_ae.reshape((len(x_train_ae), np.prod(x_train_ae.shape[1:])))\n",
    "x_test_ae = x_test_ae.reshape((len(x_test_ae), np.prod(x_test_ae.shape[1:])))\n",
    "x_val_ae = x_val_ae.reshape((len(x_val_ae), np.prod(x_val_ae.shape[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the autoencoder to reconstruct MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model to use a per-pixel binary crossentropy loss, and Adadelta optimizer\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "autoencoder.fit(x_train_ae, x_train_ae, epochs=10, batch_size=256,\n",
    "                shuffle=True, validation_data=(x_val_ae, x_val_ae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualize the reconstructed inputs and the encoded representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the encoder and decoder as separate networks\n",
    "encoder = Model(input_img, encoded)\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "encoded_imgs = encoder.predict(x_test_ae)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "# use Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_ae[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the autoencoder is not fully converged yet as it was trained on only 10 epochs. You may want to try and run it for longer to get better digit reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use a simple DNN with 2 layers to classify MNIST dataset and then we will compare its performance with CNN in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building DNN network, we need to load MNIST data and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "num_train = 6000 # Max is 60000\n",
    "num_test = 1000 # Max is 5000\n",
    "num_val = 1000 # Max is 5000\n",
    "# Only keep a subset of the data\n",
    "x_train_nn = x_train[:num_train,:,:,None]\n",
    "y_train_nn = y_train[:num_train]\n",
    "x_test_nn = x_test[:num_test,:,:,None]\n",
    "y_test_nn = y_test[:num_test]\n",
    "x_val_nn = x_test[:num_val,:,:,None]\n",
    "y_val_nn = y_test[:num_val]\n",
    "\n",
    "x_train_nn = x_train_nn.astype('float32')\n",
    "x_test_nn = x_test_nn.astype('float32')\n",
    "x_val_nn = x_val_nn.astype('float32')\n",
    "x_train_nn /= 255.\n",
    "x_test_nn /= 255.\n",
    "x_val_nn /= 255.\n",
    "# Convert class vectors to binary class matrices using keras.utils.to_categorical function \n",
    "y_train_nn = # TODO\n",
    "y_test_nn =  # TODO\n",
    "y_val_nn =  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dnn = x_train_nn.reshape(6000,784)\n",
    "x_test_dnn = x_test_nn.reshape(1000,784)\n",
    "x_val_dnn = x_val_nn.reshape(1000,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a simple 2 layer DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense # Dense layers are \"fully connected\" layers\n",
    "from keras.models import Sequential # Documentation: https://keras.io/models/sequential/\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='sigmoid', input_shape=x_train_dnn.shape[1:]))\n",
    "# TODO: add the last layer as softmax activation \n",
    "model.add(# TODO)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_dnn = model.fit(x_train_dnn, y_train_nn, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_val_dnn, y_val_nn), shuffle=True)\n",
    "\n",
    "plt.plot(history_dnn.history['acc'])\n",
    "plt.plot(history_dnn.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN topology\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train_nn.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Fit model to data\n",
    "epochs = 3\n",
    "print(epochs)\n",
    "history_cnn1 = model.fit(x_train_nn, y_train_nn, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_val_nn, y_val_nn), shuffle=True)\n",
    "plt.plot(history_cnn1.history['acc'])\n",
    "plt.plot(history_cnn1.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_nn, y_test_nn, batch_size=32, verbose=1)\n",
    "print(\"\\nTest accuracy is {}%\".format(100.0*score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN gives a better results than DNN but it takes a significant long time to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding dropout and pooling to the CNN\n",
    "Max pooling and dropouts are tricks that improve deep neural networks.\n",
    "\n",
    "Max pooling (http://yann.lecun.com/exdb/publis/pdf/boureau-icml-10.pdf) speeds up CNN training and encourages CNN to learn translation-invariant features.\n",
    "Dropout (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) is a regularisation technique preventing the model from overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN topology\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train_nn.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(# TODO: add Pooling layer size (2,2))\n",
    "model.add(# TODO: add drop out layer, change the value of dropout, and discuss the differences)\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(# TODO: add Pooling layer size (2,2))\n",
    "model.add(# TODO: add drop out layer, change the value of dropout, and discuss the differences)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(# TODO: add drop out layer, change the value of dropout, and discuss the differences)\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Fit model to data\n",
    "epochs = 3\n",
    "history_cnn2 = model.fit(x_train_nn, y_train_nn, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_val_nn, y_val_nn), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn2.history['acc'])\n",
    "plt.plot(history_cnn2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model score on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_nn, y_test_nn, batch_size=32, verbose=1)\n",
    "print(\"\\nTest accuracy is {}%\".format(100.0*score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
