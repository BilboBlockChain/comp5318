{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 5 - Polynomial Regression, Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2019**\n",
    "\n",
    "**Objectives:**\n",
    "* To learn about non-linear function approximation using polynomial features. \n",
    "* To learn about Overfitting, Underfitting\n",
    "* To learn about the effect of regularization in order to prevent overfitting.\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab5.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab5.ipynb\" file\n",
    "* Complete exercises in \"lab5.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran \n",
    "\n",
    "Tutors: Fengxiang He, Shaojun Zhang, Fangzhou Shi, Yang Lin, Iwan Budiman, Zhiyi Wang, Canh Dinh, Yixuan Zhang, Rui Dong, Haoyu He, Dai Hoang Tran, Peibo Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate nonlinear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nTrain = 30\n",
    "nTest = 20\n",
    "np.random.seed(1)\n",
    "xTrain = np.random.rand(nTrain, 1)*5\n",
    "yTrain = 3*(xTrain -2) * (xTrain - 3)*(xTrain-4) +  10 * np.random.randn(nTrain, 1)\n",
    "xTest = (np.random.rand(nTest,1) -1/8) *8\n",
    "yTest = 3*(xTest -2) * (xTest - 3)*(xTest-4) +  10*np.random.randn(nTest, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline\n",
    "pl.scatter(xTrain, yTrain,marker='.', color = 'b',label = 'Training samples')\n",
    "pl.scatter(xTest, yTest, marker='*', color='r',label = 'Testing samples')\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('y')\n",
    "pl.legend(loc='lower right')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardization/normalization dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardData(z, mean, std): \n",
    "    z_standard = (z - mean)/ std\n",
    "    return z_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_n = standardData(xTrain,xTrain.mean(),xTrain.std())\n",
    "yTrain_n = standardData(yTrain,yTrain.mean(),yTrain.std())\n",
    "xTest_n  = standardData(xTest,xTrain.mean(),xTrain.std())\n",
    "yTest_n  = standardData(yTest,yTrain.mean(),yTrain.std())\n",
    "bias = 0.01\n",
    "xMin = xTrain_n.min() - bias\n",
    "xMax = xTrain_n.max() + bias\n",
    "if(xTrain_n.min() > xTest_n.min()):\n",
    "    xMin = xTest_n.min() - bias\n",
    "if(xTrain_n.max() < xTest_n.max()):\n",
    "    xMax = xTest_n.max() + bias\n",
    "\n",
    "pl.scatter(xTrain_n, yTrain_n,marker='.', color = 'b',label = 'Training samples')\n",
    "pl.scatter(xTest_n, yTest_n, marker='*', color='r',label = 'Testing samples')\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('y')\n",
    "pl.legend(loc='lower right')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Polynomial Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x), and has been used to describe nonlinear phenomena such as the growth rate of tissues,[1] the distribution of carbon isotopes in lake sediments,[2] and the progression of disease epidemi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Objective***: Estimate (a.k.a. predict or query) outputs for unknown inputs, given the training data $\\{\\mathbf{x},\\mathbf{y}\\}$ with datasize $n$.\n",
    "\n",
    "We can use a nonlinear mapping:\n",
    " \\begin{equation}\n",
    "    \\phi(x): x \\in R^D \\leftarrow z \\in R^M \n",
    "\\end{equation}\n",
    "$\\phi(x)$ is a polynomial feature matrix\n",
    "* M is dimensionality of new features z (or $\\Phi(x)$)\n",
    "* M could be greater than, less than, or equal to D\n",
    "Consider the non linear model with:\n",
    "\\begin{equation}\n",
    "\\phi(x) = \\begin{bmatrix}\n",
    "1\\\\\n",
    "x\\\\\n",
    "x^2\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x^M\\\\\n",
    "\\end{bmatrix} \n",
    "\\rightarrow  f(x)= w^T \\phi(x) = w_0 +  w_1 x + w_2 x^2 + w_3 x^3 + ... + w_m x^m = w_0 + \\sum_{m=1}^M w_mx^m\n",
    "\\end{equation}\n",
    "\n",
    "Residual sum of squares:\n",
    "\\begin{equation}\n",
    "\\sum_n[w^T \\phi(x_n)-y_n]^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The LMS solution can be formulated with the new design matrix:\n",
    "\\begin{equation}\n",
    "  \\mathbf{\\Phi} = \\begin{bmatrix}\n",
    "  \\phi(x_1)^T\\\\\n",
    "  \\phi(x_2)^T\\\\\n",
    "  .\\\\\n",
    "  .\\\\\n",
    "  .\\\\\n",
    "  \\phi(x_N)^T\n",
    "  \\end{bmatrix} =  \\begin{bmatrix}\n",
    "  x_1^0 & x_1^1 & \\cdots & x_1^m \\\\\n",
    "  x_2^0 & x_2^1 & \\cdots & x_2^m \\\\\n",
    "  x_3^0 & x_3^1 & \\cdots & x_3^m \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  x_N^0 & x_N^1 & \\cdots & x_N^m\n",
    " \\end{bmatrix} \\in R^{NxM},  \\quad \n",
    "  \\mathbf{w^{LSM}} = (\\mathbf{\\Phi}^{\\top}\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi^{\\top} y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Complete \"TODO:\" sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Python method to generate the polynomial feature matrix. It will be used to transform $\\mathbf{x}$ to $\\mathbf{\\phi(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_mat(x_in, m):\n",
    "    # TODO:\n",
    "    ϕ = \n",
    "    return ϕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train:** Evaluate $\\phi(x)$ and hence determine the best weights $\\mathbf{w^{LSM}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "ϕ_x = generate_feature_mat(xTrain_n, m)\n",
    "\n",
    "#TODO:\n",
    "w_lsm =  # complete this\n",
    "\"\"\"\n",
    "Attention! - use np.linalg.pinv instead of np.linalg.inv. \n",
    "This is because, when a matrix is singular, we may have to obtain the Moore-Penrose pseudoinverse,\n",
    "especially when the polynomial order is very high. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 100 query inputs $\\mathbf{x}_q$ from xMin,xMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_q = np.linspace(xMin,xMax,100)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict:** \n",
    "Evaluate $\\mathbf{\\phi(x_q)}$ and hence determine outputs $\\hat{\\mathbf{y}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ_x_q = generate_feature_mat(x_q, m) #generate feature matrix for x_q\n",
    "y_hat = ϕ_x_q.dot(w_lsm) #estimated outputs (N_qx1 matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scatter(xTrain_n, yTrain_n,marker='.', color = 'b',label = 'Training samples')\n",
    "pl.scatter(xTest_n, yTest_n, marker='*', color='r',label = 'Testing samples')\n",
    "pl.plot(x_q, y_hat,label= \"m =\" + str(m))\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('y')\n",
    "pl.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Perform regression for $m \\in \\{1, 3, 4, 10\\}$. Identify the models that **under-fit** and **over-fit**. Discuss how to choose a suitable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vals = np.array([1,3,4,10])\n",
    "pl.figure(figsize=(10,10))\n",
    "pl.scatter(xTrain_n, yTrain_n, marker='.', color='b')\n",
    "pl.scatter(xTest_n, yTest_n, marker='*', color='r')\n",
    "for m in m_vals:\n",
    "    ϕ_x = generate_feature_mat(xTrain_n,m) \n",
    "    w_lsm = (np.linalg.pinv(ϕ_x.T.dot(ϕ_x))).dot(ϕ_x.T.dot(yTrain_n))\n",
    "    ϕ_x_q = generate_feature_mat(x_q, m)\n",
    "    y_hat = ϕ_x_q.dot(w_lsm)\n",
    "    pl.plot(x_q, y_hat, label='m={}'.format(m))\n",
    "pl.legend(loc='lower right')\n",
    "pl.ylim([-4,8])\n",
    "pl.title(\"m =1: Underfitting, m = 3: Good fit, m = 10: Overfitting\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tikhonov ($\\mathrm{L}^2$) regularization can be used to overcome overfitting. \n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{w^{LMS}} = (\\mathbf{\\Phi}^{\\top}\\mathbf{\\Phi} + \\lambda \\mathbf{I})^{-1}\\mathbf{\\Phi^{\\top} y}\n",
    "\\end{equation}\n",
    "\n",
    "In other words, we can cosider this as a minimization problem in the following form.\n",
    "\\begin{equation}\n",
    " \\mathbf{w^{LMS}} = \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\| \\mathbf{w^T\\phi(x) - y}\\|_2^2\n",
    " + \\lambda \\| \\mathbf{w} \\|_2^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2** : Define the loss function between real output and predicted ouput using mean_quare_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def loss(yPre,y):\n",
    "    loss = mean_squared_error(y,yPre)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**: Test the following code for different values of the regularization parameter $\\lambda$ and obsever the changing of traning loss and testing loss. Chosing the best value for regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_val = np.linspace(0,10,100)[:,np.newaxis]\n",
    "m = 3\n",
    "trainLoss = []\n",
    "testLoss = []\n",
    "for lamb in lambda_val:\n",
    "    ϕ_x_train = generate_feature_mat(xTrain_n, m)\n",
    "    ϕ_x_test = generate_feature_mat(xTest_n, m)\n",
    "    # TODO:\n",
    "    w_lsm = #\n",
    "    trainLoss.append(loss(ϕ_x_train.dot(w_lsm),yTrain_n))\n",
    "    testLoss .append(loss(ϕ_x_test.dot(w_lsm),yTest_n))\n",
    "pl.plot(lambda_val, trainLoss, marker='.', color='b',label = \"Training Loss\")\n",
    "pl.plot(lambda_val, testLoss, marker='.', color='r',label = \"Test Loss\")\n",
    "pl.legend()\n",
    "pl.xlabel('Lambda')\n",
    "pl.ylabel('Loss')\n",
    "pl.show()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
