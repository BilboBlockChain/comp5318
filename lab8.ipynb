{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 8 - Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2019**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To learn about Support Vector Machines (SVM)\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab8.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab8.ipynb\" file\n",
    "* Complete exercises in \"lab8.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran\n",
    "\n",
    "Tutors: Fengxiang He, Shaojun Zhang, Fangzhou Shi, Yang Lin, Iwan Budiman, Zhiyi Wang, Canh Dinh, Yixuan Zhang, Rui Dong, Haoyu He, Dai Hoang Tran, Peibo Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "np.random.seed(22)\n",
    "\n",
    "means = [[2, 2], [4, 2]]\n",
    "cov = [[.3, .2], [.2, .3]]\n",
    "N = 20\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N) # each row is a data point \n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X = np.concatenate((X0, X1))\n",
    "y = np.concatenate((np.ones(N), -np.ones(N)))\n",
    "\n",
    "plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = 1)\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = 1)\n",
    "plt.axis('equal')\n",
    "plt.ylim(0, 4)\n",
    "plt.xlim(2, 4)\n",
    "plt.xlabel('$x_1$', fontsize = 20)\n",
    "plt.ylabel('$x_2$', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Solving SVM problem using Hinge Loss Function and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Hinge Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Softmax function:\n",
    "Input: $\\textbf{x} \\in \\textbf{R}^{D}$\n",
    "\n",
    "$$\n",
    "\\textbf{J}(w,b) = min_{\\textbf{w},b} \\sum_n max(0,1 - y_n [ \\textbf{w}^T\\textbf{x}_n +b ]) + \\frac{\\lambda}{2} ||\\textbf{w}||^2 \n",
    "$$\n",
    "\n",
    "Can solve using gradient descent to get the optimal $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gradient of hinge loss function:\n",
    "\n",
    "if $1-y_n[ \\textbf{w}^T\\textbf{x}_n +b] \\geq 0$:\n",
    "$$\n",
    "\\nabla \\textbf{J}_w(w,b) = -y_n\\textbf{x}_n + \\lambda \\textbf{w}\n",
    "$$\n",
    "$$\n",
    "\\nabla \\textbf{J}_b(w,b) = -y_n\n",
    "$$\n",
    "\n",
    "else:\n",
    "\n",
    "$$\n",
    "\\nabla \\textbf{J}_w(w,b) = \\lambda \\textbf{w}\n",
    "$$\n",
    "$$\n",
    "\\nabla \\textbf{J}_b(w,b) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X0.min(),X0.max())\n",
    "print(X1.min(),X1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 100\n",
    "lam = 1./C\n",
    "\n",
    "def loss(X, y, w, b): \n",
    "    \"\"\"\n",
    "    X.shape = (2N, d), y.shape = (2N,), w.shape = (d,), b is a scalar \n",
    "    \"\"\"\n",
    "    z = X.dot(w) + b # shape (2N,)\n",
    "    yz = y*z\n",
    "    return #TODO: Define the loss function\n",
    "\n",
    "def grad(X, y, w, b):\n",
    "    z = X.dot(w) + b # shape (2N,)\n",
    "    yz = y*z         # element wise product, shape (2N,)\n",
    "    active_set = np.where(yz <= 1)[0] # consider 1 - yz >= 0 only \n",
    "    _yX = - X*y[:, np.newaxis]   # each row is y_n*x_n \n",
    "    grad_w = #TODO Calculate gradient of w\n",
    "    grad_b = #TODO Calculate gradient of b\n",
    "    return (grad_w, grad_b)\n",
    "\n",
    "def num_grad(X, y, w, b):\n",
    "    eps = 1e-10\n",
    "    gw = np.zeros_like(w)\n",
    "    gb = 0\n",
    "    for i in range(len(w)):\n",
    "        wp = w.copy()\n",
    "        wm = w.copy()\n",
    "        wp[i] += eps \n",
    "        wm[i] -= eps \n",
    "        gw[i] = (loss(X, y, wp, b) - loss(X, y, wm, b))/(2*eps)\n",
    "    gb = (loss(X, y, w, b + eps) - loss(X, y, w, b - eps))/(2*eps)\n",
    "    return (gw, gb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = .1*np.random.randn(X.shape[1])\n",
    "b = np.random.randn()\n",
    "(gw0, gb0) = grad(X, y, w, b)\n",
    "(gw1, gb1) = num_grad(X, y, w, b)\n",
    "print('grad_w difference = ', np.linalg.norm(gw0 - gw1))\n",
    "print('grad_b difference = ', np.linalg.norm(gb0 - gb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmarginSVM_gd(X, y, w0, b0, eta):\n",
    "    w = w0\n",
    "    b = b0\n",
    "    it = 0 \n",
    "    while it < 10000:\n",
    "        it = it + 1\n",
    "        (gw, gb) = #TODO Get gradient from grad function\n",
    "        w -= eta*gw\n",
    "        b -= eta*gb\n",
    "        if (it % 1000) == 0:\n",
    "            print('iter %d' %it + ' loss: %f' %loss(X, y, w, b))\n",
    "    return (w, b)\n",
    "\n",
    "w0 = .1*np.random.randn(X.shape[1]) \n",
    "b0 = .1*np.random.randn()\n",
    "lr = 0.05\n",
    "(w_hinge, b_hinge) = softmarginSVM_gd(X, y, w0, b0, lr)\n",
    "print('w_hinge = ', w_hinge)\n",
    "print('b_hinge = ', b_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myplot(X0, X1, w, b, filename, tit):\n",
    "    with PdfPages(filename) as pdf:\n",
    "        fig, ax = plt.subplots()\n",
    "        w0 = w[0]\n",
    "        w1 = w[1]\n",
    "        x1 = np.arange(-10, 10, 0.1)\n",
    "        y1 = -w0/w1*x1 - b/w1\n",
    "        y2 = -w0/w1*x1 - (b-1)/w1\n",
    "        y3 = -w0/w1*x1 - (b+1)/w1\n",
    "        plt.plot(x1, y1, 'k', linewidth = 3)\n",
    "        plt.plot(x1, y2, 'k')\n",
    "        plt.plot(x1, y3, 'k')\n",
    "\n",
    "        # equal axis and lim\n",
    "        plt.axis('equal')\n",
    "        plt.ylim(0, 4)\n",
    "        plt.xlim(2, 4)\n",
    "\n",
    "        # hide ticks \n",
    "        cur_axes = plt.gca()\n",
    "        cur_axes.axes.get_xaxis().set_ticks([])\n",
    "        cur_axes.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "        plt.xlabel('$x_1$', fontsize = 20)\n",
    "        plt.ylabel('$x_2$', fontsize = 20)\n",
    "        plt.title('Solution: ' + tit, fontsize = 20)\n",
    "\n",
    "        plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n",
    "        plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n",
    "        pdf.savefig()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(X0, X1, w_hinge, b_hinge, 'svm_hinge.pdf', 'GD using Hinge Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solving SVM problem using Lagrange Duality and KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function:**\n",
    "$$\n",
    "\t\\textbf{J}(w,b,\\xi_n) = min_{\\textbf{w},b,\\xi} \\frac{1}{2}||\\textbf{w}||^2 + {C}\\sum \\xi_n\n",
    "$$\n",
    "$$\n",
    "s.t. y_n [\\textbf{w}^T\\textbf{x}_n +b ] \\geq 1 - \\xi_n\n",
    "$$ and $\\xi_n \\geq 0 $  while $C$ is constant and $ C> 0$\n",
    "\n",
    "**Lagrangian:**\n",
    "\n",
    "$$\n",
    "L(\\textbf{w},b,\\{\\xi_n\\},\\{\\alpha_n\\},\\{\\lambda_n\\}) = {C}\\sum \\xi_n +  \\frac{1}{2}||\\textbf{w}||^2 - \\sum_n \\lambda_n \\xi_n + \\sum_n \\alpha_n \\{ 1- y_n [\\textbf{w}^T\\textbf{x}_n +b ] - \\xi_n\\}\n",
    "$$\n",
    "\n",
    "$$\\alpha_n \\geq 0,\\lambda_n \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking derivatives of $L$ st $b$,$\\xi_n$, $\\textbf{w}$ respectively we have:\n",
    "$$\n",
    "\\sum_n \\alpha_ny_n = 0\n",
    "$$\n",
    "$$\n",
    "C -\\lambda_n - \\alpha_n = 0\n",
    "$$\n",
    "\n",
    "\n",
    "**and the solution for w**:\n",
    "$$\n",
    "\\textbf{w} = \\sum_n \\alpha_n y_n \\textbf{x}_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the dual problem to find $\\alpha$:** \n",
    "\n",
    "$$\n",
    "   max_{\\alpha} g(\\alpha_n, \\lambda_n) = \\sum \\alpha_n - \\frac{1}{2}\\sum_{m,n}y_my_n\\alpha_m\\alpha_n\\textbf{x}_m^T \\textbf{x}_n\n",
    "$$\n",
    "$$\n",
    "s.t. \\alpha_n > 0 , \\sum \\alpha_ny_n = 0, C- \\lambda_n - \\alpha_n = 0,  \\lambda_n \\geq 0\n",
    "$$\n",
    "and KKT we have:\n",
    "\n",
    "$\\xi_n = 0$ and $0 < \\alpha_n < C$ when $y_n [\\textbf{w}^T\\textbf{x}_n +b ] = 1$\n",
    "\n",
    "$\\xi_n > 0$ and $\\alpha_n  = C$ if $y_n [\\textbf{w}^T\\textbf{x}_n +b ] < 1$ \n",
    "\n",
    "**Solution for b:**\n",
    "$$\n",
    "b = y_n - \\sum_m \\alpha_m y_m \\textbf{x}_m^T \\textbf{x}_n\n",
    "$$\n",
    "\n",
    "If $0 < \\alpha_n < C$ and $y_n \\in \\{-1,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf for information about Solving a quadratic program using python\n",
    "\n",
    "Assume that matrix:\n",
    "$V = [y_1\\textbf{x}_1,y_2\\textbf{x}_2,...,y_n\\textbf{x}_n]$ and $K = V^TV$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Step 1: Finding alpha \n",
    "V = np.concatenate((X0, -X1), axis = 0) # V[n,:] = y[n]*X[n]\n",
    "\n",
    "# build K\n",
    "K = matrix(V.dot(V.T))\n",
    "p = matrix(-np.ones((2*N, 1)))\n",
    "\n",
    "# build A, b, G, h \n",
    "G = matrix(np.vstack((-np.eye(2*N), np.eye(2*N))))\n",
    "h = np.vstack((np.zeros((2*N, 1)), C*np.ones((2*N, 1))))\n",
    "h = matrix(np.vstack((np.zeros((2*N, 1)), C*np.ones((2*N, 1)))))\n",
    "A = matrix(y.reshape((-1, 2*N))) \n",
    "b = matrix(np.zeros((1, 1))) \n",
    "solvers.options['show_progress'] = False\n",
    "\n",
    "#  solving the dual problem (finding: alpha)\n",
    "sol = solvers.qp(K, p, G, h, A, b)\n",
    "al = np.array(sol['x']).reshape(2*N) # alpha vector \n",
    "\n",
    "# Step 2: finding w and b using above solution\n",
    "w_dual = # To Do; Caculate W using dual methods\n",
    "b_dual = # To Do; Caculate b using dual methods\n",
    "print('w_dual = ', w_dual)\n",
    "print('b_dual = ', b_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(X0, X1, w_dual, b_dual, 'svm_b_dual.pdf', 'GD using Duality and KKT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solving by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "C = 100 # lambda = 0.01\n",
    "#TODO: Using SVC from sklearn to find w and b\n",
    "\n",
    "w_sklearn = clf.coef_.reshape(-1, 1)\n",
    "b_sklearn = clf.intercept_[0]\n",
    "\n",
    "print('w_sklearn = ', w_sklearn.T)\n",
    "print('b_sklearn = ', b_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(X0, X1, w_sklearn, b_sklearn, 'svm_b_sklearn.pdf', 'GD using sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ddb97c5eae1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'API_Version_Date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"2019-09-30\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresults_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"Predictions\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfinal_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mresults_dic\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mversion\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "version = {'API_Version_Date': \"2019-09-30\"}\n",
    "results_dic = {\"Predictions\": [1,2,3,4]}\n",
    "final_results = {results_dic,  version}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
