{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5138 Assignment 1 - Sem 2 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script assumes images_training.h5, labels_training.h5, images_testing.h5 and labels_testing_2000.h5 are in a child folder called 'Input' and that there is a child folder called 'Output' to generate the predicted label .h5 into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with h5py.File('./Input/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "\n",
    "# using H['datatest'], H['labeltest'] for test dataset.\n",
    "print(data_train.shape,label_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we seperate 5000 records from the train data set to use as a validation set. We verify that the last 5000 records of the training set are a representative sample of the 10 classes in Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 507, 1: 478, 2: 523, 3: 511, 4: 467, 5: 508, 6: 499, 7: 514, 8: 490, 9: 503}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(label_train[25000:,], return_counts=True)\n",
    "print(dict(zip(unique, counts))) #display counts of classes in candidate validation set\n",
    "\n",
    "#separate training and validation set \n",
    "data_val = data_train[25000:,]\n",
    "data_partial_train = data_train[:25000,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform the same train/val split on the labels training set. We then one hot encode the labels vector, changing its shape from size (samples,) to (samples, classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 784)\n"
     ]
    }
   ],
   "source": [
    "#one hot encode y for softmax output \n",
    "def oneHot(y):\n",
    "    zeroesY = np.zeros((y.size, y.max() + 1)) #generate matrix of zeroes shape (samples, classes)\n",
    "    zeroesY[np.arange(y.size), y] = 1 #insert value 1 at label y's scalar class value\n",
    "    return zeroesY\n",
    "\n",
    "label_val = oneHot(label_train[25000:,]) \n",
    "label_partial_train = oneHot(label_train[: 25000,])\n",
    "\n",
    "print(data_partial_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then reshape the train and val data tensors of shape (samples,28,28) to matrices of shape (samples, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 784)\n",
      "(25000, 784) (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(data_partial_train.shape)\n",
    "\n",
    "\n",
    "def pre_proc_mat(A, dims = 784):\n",
    "    flat_A = A.reshape(A.shape[0],dims)\n",
    "    return flat_A\n",
    "\n",
    "\n",
    "data_partial_train = pre_proc_mat(data_partial_train)\n",
    "data_val = pre_proc_mat(data_val)\n",
    "\n",
    "print(data_partial_train.shape, data_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we define the SVD fitting function. This takes in two parameters (data matrix, number of components), performs SVD decomposition and returns the right singular value V of shape (m,k), where m is the dimension of features and k is the choice of number of leading components from the SVD decomposition. When k < m dimension reduction has been performed:\n",
    "$$\n",
    "X_{n, m} V_{m, k}=U S V^{T} V=U_{n, k} S_{k, k}\n",
    "$$\n",
    "We perform this fitting procedure on the training set only (taking 150 components) and use the same V for the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 150)\n",
      "(25000, 150) (5000, 150)\n"
     ]
    }
   ],
   "source": [
    "def svd_fit(A, comps):   \n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    #the matrix XV = UD therefore only need right singular value for decomposition\n",
    "    V_tilde = Vt.T[:,0:comps] #create parameter to pick number of leading components to take from V\n",
    "    return V_tilde\n",
    "\n",
    "v = svd_fit(flat_data_partial_train, 150)\n",
    "\n",
    "print(v.shape)\n",
    "\n",
    "#take dot product of train and val set with V to perform dimension reduction\n",
    "dim_partial_train = data_partial_train.dot(v)\n",
    "dim_data_val = data_val.dot(v)\n",
    "\n",
    "print( dim_partial_train.shape, dim_data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move onto the building blocks of the softmax classifer. <br>\n",
    "<br>\n",
    "First we define the softmax function. it takes in the matrix X.W which has shape (samples,classes) and produces a matrix of shape (samples, classes) that sums to one per sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X): \n",
    "    eX = np.exp(X)\n",
    "    A = eX / eX.sum(axis = 1, keepdims = True) #here we sum along classes per sample and broadcast this as the denominator\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the loss function taking in the paramaters (data matrix X, one hot encoded labels y, weight matrix W, regulariser penalty l): \n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{w}, l)=-\\sum_{n} \\sum_{k} y_{n k} \\log \\left(\\sigma_{\\boldsymbol{w}}(\\boldsymbol{x})\\right)+\\frac{l}{2}\\|\\boldsymbol{w}\\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "Where sigma denotes the softmax transformation. The sum over all samples and all classes is captured in np.sum() over the matrix with shape (samples,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define softmax loss \n",
    "def softmax_loss(X, Y, W, l):\n",
    "    A = softmax(X.dot(W))\n",
    "    n = len(X)\n",
    "    snorm = np.linalg.norm(W)**2\n",
    "    return (-1/n) * np.sum(Y * np.log(A)) + (l/2)*snorm #np.sum over the matrix captures summing over both classes and samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the direct gradient of the above loss function by creating a function that takes in the same paramaters:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{n}\\left(\\sigma_{w}\\left(\\boldsymbol{x}_{n}\\right)-y_{n}\\right) \\boldsymbol{x}^{T}+l \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "The output takes the shape (features, classes), preparing to be minused from the identical shaped weight vector W.\n",
    "We slightly change the above formulation to compute the gradient over the whole data matrix with shape (samples, features). This requires a dot product with the matrix (A-Y) that has shape (samples, classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define softmax gradient \n",
    "def softmax_grad(X, Y, W, l):\n",
    "    A = softmax(X.dot(W))  \n",
    "    n = len(X) \n",
    "    return  (np.dot(X.T,(A - Y))/n) + l*W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the softmax loss and gradients defined we can now define the fitting procedure. Here we employ batch gradient descent. This procedure involves choosing a randomly sampled without replacement batch size iteratively to perform gradient operations on. This value times a learning rate alpha is then minused from a weight vector. This procedure is then repeated over the dataset for a certain number of epochs. \n",
    "\n",
    "This function takes in a data matrix, the one hot encoded labels, a vector to optimise, a ridge penalty l, a number of epochs to optmise over and a number of batches per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_fit(X, Y, W, l, alpha, epoch,  batch):\n",
    "    n = len(X) #sample size\n",
    "    loss_hist = [softmax_loss(X, Y, W, l)] #generate intital loss history\n",
    "    steps = int(np.ceil(n/batch)) #define the number of steps per epoch, determined by how many batches fit in the sample\n",
    "    for ep in range(epoch): \n",
    "        p_ids = np.random.permutation(n) #generate a set of shuffled ids the size of the sample\n",
    "        p_X = X[p_ids] #apply this shuffled id to data matrix\n",
    "        p_Y = Y[p_ids] #apply this shuffled id to label matrix\n",
    "        for i in range(steps): \n",
    "            # get the i-th batch\n",
    "            X_batch = p_X[i:i + batch, :] #subset data matrix to size batch starting at step point i\n",
    "            Y_batch = p_Y[i:i + batch] #perform same operation to label matrix\n",
    "            W -=  alpha * softmax_grad(X_batch, Y_batch, W, l) #minus gradient for current w times learning rate from next w\n",
    "        loss_hist.append(softmax_loss(X, Y, W, l)) #persist loss over runs\n",
    "    return W, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the softmax fitting algorithm we now generate initial values for the weight vector with shape (features, classes) and run the fitting algorithm on our chosen hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_rand = np.random.randn(dim_partial_train.shape[1], len(label_partial_train[1]))\n",
    "W, loss_hist = softmax_fit(dim_partial_train, \n",
    "                                      label_partial_train , \n",
    "                                      W_rand, \n",
    "                                      epoch = 1500, \n",
    "                                      batch = 1000,\n",
    "                                      alpha = 0.1, \n",
    "                                      l = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the weight paramaters we can report the maximum probability from the softmax function to retrieve our model predictions. Accuracy is then assessed by summing where the condition for  predicted values and actual values is equivalent and dividing this by the total length of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions from softmax\n",
    "def pred(W, X):\n",
    "    A = softmax(X.dot(W))\n",
    "    return np.argmax(A, axis = 1)\n",
    "\n",
    "#accuracy\n",
    "def accuracy(y_pred,y):\n",
    "    results = pd.DataFrame({'label': y, 'answer': y == y_pred})\n",
    "    return results['answer'].sum()/len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = pred(W,dim_data_val)\n",
    "val_result = accuracy(y_pre,label_train[25000:,])\n",
    "print(f'Validation set accuracy is {val_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing step can then be repeated to generate predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./Input/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test.shape)\n",
    "\n",
    "#subset to labelled set\n",
    "data_test_sub = data_test[:2000,:]\n",
    "\n",
    "data_test_sub = pre_proc_mat(data_test_label)\n",
    "\n",
    "dim_data_test_sub = data_test_sub.dot(v)\n",
    "\n",
    "print(dim_data_test_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = accuracy(pred(W, dim_data_test_sub ), label_test)\n",
    "\n",
    "print(f'Test set accuracy is {test_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Full Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./Input/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])\n",
    "    \n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pre_proc_mat(data_test)\n",
    "print(data_test.shape)\n",
    "\n",
    "dim_data_test = data_test.dot(v)\n",
    "\n",
    "output = pred(W, dim_data_test)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./Output/predicted_labels.h5','w') as H:\n",
    "    H.create_dataset('output',data=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy as sp\n",
    "from math import sqrt\n",
    "import pandas\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File('./Input/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "    \n",
    "with h5py.File('./Input/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])\n",
    "\n",
    "print(data_train.shape,label_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_train = data_train.reshape((data_train.shape[0], 28, 28))\n",
    "data_test=data_test.reshape((data_test.shape[0], 28, 28))\n",
    "plt.imshow(data_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"class \" + str(label_train[0]) + \": T-shirt/Top\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values are continuous float, we need to bin the data, i.e. put the values into categories based on their value. We did this by multiplying all values by 10 and converting them into integers and as such we created 10 categories/bins labelled as 0 to 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#convert the array value into int in order to bin the data\n",
    "def Cover_to_int(array_x):\n",
    "\tx=np.multiply(array_x, 10)\n",
    "\treturn x.astype(int)\n",
    "\n",
    "#create val set\n",
    "data_val = Cover_to_int(data_train[25000:30000,])\n",
    "partial_train = Cover_to_int(data_train[:25000,])\n",
    "\n",
    "#create val labels\n",
    "label_val = label_train[25000:30000,].astype('int')\n",
    "partial_label_train = label_train[:25000,].astype('int')\n",
    "\n",
    "\n",
    "#create testing set\n",
    "data_test=Cover_to_int(data_test[:2000])\n",
    "\n",
    "print(data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data transformation,we use numpy.reduceat function to combine every 4 adjacent values on the matrix into 1 value by taking the their average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data\n",
    "def reshape_matrix(x):\n",
    "    x_1=np.add.reduceat(x,np.arange(0,28,2),1)\n",
    "    x_2=np.add.reduceat(x_1,np.arange(0,28,2),2)\n",
    "    x_3=x_2.reshape(x_2.shape[0],-1)\n",
    "    x_v=np.multiply(x_3, 0.25).astype(int)\n",
    "    return x_v\n",
    "\n",
    "flat_partial_train = reshape_matrix(partial_train)\n",
    "flat_data_val = reshape_matrix(data_val)\n",
    "flat_data_test = reshape_matrix(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the date distribution between bins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values :  [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Occurrence Count :  [2484948  234889  247824  264361  274948  275550  315742  365818  359926\n",
      "   75954      40]\n"
     ]
    }
   ],
   "source": [
    "uniqueValues, Count = np.unique(flat_partial_train, return_counts=True)\n",
    "print(\"Unique Values : \" , uniqueValues)\n",
    "print(\"Occurrence Count : \", Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the count is classs 9 and 10 are significantly lower than other bins, they are combined into bin 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_partial_train[flat_partial_train > 8] = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine training data with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covert the matrix into 2 dimension\n",
    "train_label_reshaped=partial_label_train.reshape(partial_label_train.shape[0],1)\n",
    "\n",
    "#add the label data as the last feature to the train data\n",
    "Train_data = np.hstack((flat_partial_train,train_label_reshaped))\n",
    "\n",
    "#covert the matrix into 2 dimension\n",
    "test_label_reshaped=label_val.reshape(label_val.shape[0],1)\n",
    "\n",
    "#add the label data as the last feature to the train data\n",
    "Test_data = np.hstack((flat_data_val,test_label_reshaped))\n",
    "\n",
    "#covert the matrix into 2 dimension\n",
    "access_label_reshaped=label_test.reshape(label_test.shape[0],1)\n",
    "\n",
    "#add the label data as the last feature to the train data\n",
    "Access_data = np.hstack((flat_data_test,access_label_reshaped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset according the the feature (ref) and criterion (value)\n",
    "def data_split(ref, value, dataset):\n",
    "\tl, r = list(), list()   \n",
    "    #check all rows in dataset\n",
    "\tfor row in dataset:\n",
    "        #if a value in the dataset is less then the criterion, put it to the left node, otherwise put it to the right node\n",
    "\t\tif row[ref] < value:\n",
    "\t\t\tl.append(row)\n",
    "\t\telse:\n",
    "\t\t\tr.append(row)\n",
    "\treturn l, r\n",
    " \n",
    "# Calculate the Gini index\n",
    "def gini_index(DataSplit, classes):\n",
    "\t# count all samples at split point\n",
    "\tcount_samples = float(sum([len(group) for group in DataSplit]))\n",
    "\t# sum weighted Gini ref for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in DataSplit:\n",
    "        # calculate number of samples in the group\n",
    "\t\tsize = float(len(group))\n",
    "        # set initial score at 0\n",
    "\t\tscore = 0.0\n",
    "\t\t# avoid divide by zero by giving it 0 score directly\n",
    "\t\tif size == 0:\n",
    "\t\t\tscore = 0.0\n",
    "\t\telse:\t\t\t        \n",
    "            # score the group based on the score for each class\n",
    "\t\t\tfor class_value in classes:\n",
    "\t\t\t\tclass_sub=  [row[-1] for row in group]\n",
    "                # calculate Proportion = number of classes / number of samples in the group\n",
    "\t\t\t\tp = class_sub.count(class_value) / size\n",
    "\t\t\t\tscore += p **2\n",
    "\t\t# Gini index = [1-∑ (proportion * proportion)] * (group size/ dataset size)\n",
    "\t\tgini += (1.0 - score) * (size / count_samples)\n",
    "\treturn gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tunique_class_val = list(set(row[-1] for row in dataset))\n",
    "\tb_score=9999\n",
    "    # get the number of columns  \n",
    "\tn_feature = len(dataset[0])-1 \n",
    "\tfor ref in range(n_feature):\n",
    "        # get unique values in the column  \n",
    "\t\tunique_value=set([row[ref] for row in dataset])\n",
    "\t\tfor criteria in unique_value:\n",
    "            #skip the class 0 and 9 as they cannot split the dataset into two groups\n",
    "\t\t\tif criteria==0 or criteria==9:\n",
    "\t\t\t\tcontinue       \n",
    "\t\t\tDataSplit = data_split(ref, criteria, dataset)\n",
    "\t\t\tgini = gini_index(DataSplit, unique_class_val)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_ref = ref \n",
    "\t\t\t\tb_value = criteria \n",
    "\t\t\t\tb_score = gini \n",
    "\t\t\t\tb_DataSplit = DataSplit\n",
    "\treturn {'ref':b_ref, 'value':b_value, 'branch':b_DataSplit}\n",
    " \n",
    "# Create a terminal node value\n",
    "def terminal(group):\n",
    "\tresult_class_val = [row[-1] for row in group]\n",
    "    #use most common class value as the output for terminal node\n",
    "\toutput=max(set(result_class_val), key=result_class_val.count)\n",
    "\treturn output\n",
    " \n",
    "# Create child-node and terminal node\n",
    "def split(node, max_depth, min_leaf_size, depth):\n",
    "\tl, r = node['branch']\n",
    "    #delete node as it is not needed\n",
    "\tdel(node['branch'])\n",
    "\tlen_l=len(l)\n",
    "\tlen_r=len(r)\t\n",
    "\t# if a split is a pure split, i.e. all value in a split belong to the same group. \n",
    "    # we wouldn’t be able to further split the data.\n",
    "\tif not l or not r:\n",
    "\t\tnode['l'] = node['r'] = terminal(l + r)\n",
    "\t\treturn\n",
    "\t# if it is over the max_depth, no further split required\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['l'], node['r'] = terminal(l), terminal(r)\n",
    "\t\treturn\n",
    "\t# if leave size less than min_leaf_size, no further split required\n",
    "\tif len_l <= min_leaf_size:      \n",
    "\t\tnode['l'] = terminal(l)\n",
    "\telse:\n",
    "        # if leave size is bigger than min_leaf_size, proceed with further split\n",
    "\t\tnode['l'] = get_split(l)\n",
    "\t\tdepth+=1\n",
    "\t\tsplit(node['l'], max_depth, min_leaf_size, depth)\n",
    "\t# if leave size less than min_leaf_size, no further split required\n",
    "\tif len_r <= min_leaf_size:\n",
    "\t\tnode['r'] = terminal(r)\n",
    "\telse:\n",
    "        # if leave size is bigger than min_leaf_size, proceed with further split\n",
    "\t\tnode['r'] = get_split(r)\n",
    "\t\tdepth+=1\n",
    "\t\tsplit(node['r'], max_depth, min_leaf_size, depth)\n",
    " \n",
    " # Call all functions needed to build the decision tree\n",
    "def build_tree(train, max_depth, min_leaf_size):\n",
    "\ttree = get_split(train)\n",
    "    # set initial depth as 1 as the split starting from root\n",
    "\tsplit(tree, max_depth, min_leaf_size, 1)\n",
    "\treturn tree\n",
    " \n",
    "# Predict with decision tree\n",
    "def predict(tree, row):\n",
    "    # tree has three components: ref, value, branch\n",
    "    # the below code compares data value to criteria for split and determine if the left or right branch should be followed\n",
    "\tif row[tree['ref']] < tree['value']:\n",
    "        # check if the a ternimal node is reached\n",
    "\t\tif isinstance(tree['l'], dict):\n",
    "\t\t\treturn predict(tree['l'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn tree['l']\n",
    "\telse:\n",
    "        # check if the a ternimal node is reached\n",
    "\t\tif isinstance(tree['r'], dict):\n",
    "\t\t\treturn predict(tree['r'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn tree['r']\n",
    "\n",
    " \n",
    " # combin everything together\n",
    "def decision_tree(train, test, max_depth, min_leaf_size):\n",
    "\ttree = build_tree(train, max_depth, min_leaf_size)\n",
    "    # set prediction result as list\n",
    "\tpredictions = list()\n",
    "    # preform the prediction row by row\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(tree, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = decision_tree(Train_data,Access_data,19,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6085\n"
     ]
    }
   ],
   "source": [
    "def accuracy(pred,y):\n",
    "    Correct_count=0\n",
    "    for i in range(0,len(pred)):\n",
    "        if pred[i]==y[i]:\n",
    "            Correct_count+=1 \n",
    "    return Correct_count/len(y)\n",
    "\n",
    "Pred_Accuracy = accuracy(pred,label_test)\n",
    "print(Pred_Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def accuracy(pred,y):\n",
    "    Correct_count=0\n",
    "    for i in range(0,len(pred)):\n",
    "        if pred[i]==y[i]:\n",
    "            Correct_count+=1 \n",
    "    return Correct_count/len(y)\n",
    "\n",
    "Pred_Accuracy = accuracy(pred,label_test)\n",
    "print(Pred_Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
