{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = './Input/images_training.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6616538db59b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Input/images_training.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datatrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Input/labels_training.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = './Input/images_training.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy as sp\n",
    "from math import sqrt\n",
    "import pandas\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File('./Input/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "    \n",
    "with h5py.File('./Input/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])\n",
    "\n",
    "print(data_train.shape,label_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_train = data_train.reshape((data_train.shape[0], 28, 28))\n",
    "data_test=data_test.reshape((data_test.shape[0], 28, 28))\n",
    "plt.imshow(data_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"class \" + str(label_train[0]) + \": T-shirt/Top\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values are continuous float, we need to bin the data, i.e. put the values into categories based on their value. We did this by multiplying all values by 10 and converting them into integers and as such we created 10 categories/bins labelled as 0 to 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#convert the array value into int in order to bin the data\n",
    "def Cover_to_int(array_x):\n",
    "\tx=np.multiply(array_x, 10)\n",
    "\treturn x.astype(int)\n",
    "\n",
    "#create val set\n",
    "data_val = Cover_to_int(data_train[25000:30000,])\n",
    "partial_train = Cover_to_int(data_train[:25000,])\n",
    "\n",
    "#create val labels\n",
    "label_val = label_train[25000:30000,].astype('int')\n",
    "partial_label_train = label_train[:25000,].astype('int')\n",
    "\n",
    "\n",
    "#create testing set\n",
    "data_test=Cover_to_int(data_test[:2000])\n",
    "\n",
    "print(data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data transformation,we use numpy.reduceat function to combine every 4 adjacent values on the matrix into 1 value by taking the their average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data\n",
    "def reshape_matrix(x):\n",
    "    x_1=np.add.reduceat(x,np.arange(0,28,2),1)\n",
    "    x_2=np.add.reduceat(x_1,np.arange(0,28,2),2)\n",
    "    x_3=x_2.reshape(x_2.shape[0],-1)\n",
    "    x_v=np.multiply(x_3, 0.25).astype(int)\n",
    "    return x_v\n",
    "\n",
    "flat_partial_train = reshape_matrix(partial_train)\n",
    "flat_data_val = reshape_matrix(data_val)\n",
    "flat_data_test = reshape_matrix(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the date distribution between bins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values :  [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Occurrence Count :  [23721  2250  2313  2517  3009  3054  3201  3634  4361   939     1]\n"
     ]
    }
   ],
   "source": [
    "uniqueValues, Count = np.unique(flat_partial_train, return_counts=True)\n",
    "print(\"Unique Values : \" , uniqueValues)\n",
    "print(\"Occurrence Count : \", Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the count is classs 9 and 10 are significantly lower than other bins, they are combined into bin 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_partial_train[flat_partial_train > 8] = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine training data with lebals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covert the matrix into 2 dimension\n",
    "train_label_reshaped=partial_label_train.reshape(partial_label_train.shape[0],1)\n",
    "\n",
    "#add the label data as the last feature to the train data\n",
    "Train_data = np.hstack((flat_partial_train,train_label_reshaped))\n",
    "\n",
    "#covert the matrix into 2 dimension\n",
    "test_label_reshaped=label_val.reshape(label_val.shape[0],1)\n",
    "\n",
    "#add the label data as the last feature to the train data\n",
    "Test_data = np.hstack((flat_data_val,test_label_reshaped))\n",
    "\n",
    "#covert the matrix into 2 dimension\n",
    "access_label_reshaped=label_test.reshape(label_test.shape[0],1)\n",
    "\n",
    "#add the label data as the last feature to the train data\n",
    "Access_data = np.hstack((flat_data_test,access_label_reshaped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset according the the feature (ref) and criterion (value)\n",
    "def data_split(ref, value, dataset):\n",
    "\tl, r = list(), list()   \n",
    "    #check all rows in dataset\n",
    "\tfor row in dataset:\n",
    "        #if a value in the dataset is less then the criterion, put it to the left node, otherwise put it to the right node\n",
    "\t\tif row[ref] < value:\n",
    "\t\t\tl.append(row)\n",
    "\t\telse:\n",
    "\t\t\tr.append(row)\n",
    "\treturn l, r\n",
    " \n",
    "# Calculate the Gini index\n",
    "def gini_index(DataSplit, classes):\n",
    "\t# count all samples at split point\n",
    "\tcount_samples = float(sum([len(group) for group in DataSplit]))\n",
    "\t# sum weighted Gini ref for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in DataSplit:\n",
    "        # calculate number of samples in the group\n",
    "\t\tsize = float(len(group))\n",
    "        # set initial score at 0\n",
    "\t\tscore = 0.0\n",
    "\t\t# avoid divide by zero by giving it 0 score directly\n",
    "\t\tif size == 0:\n",
    "\t\t\tscore = 0.0\n",
    "\t\telse:\t\t\t        \n",
    "            # score the group based on the score for each class\n",
    "\t\t\tfor class_value in classes:\n",
    "\t\t\t\tclass_sub=  [row[-1] for row in group]\n",
    "                # calculate Proportion = number of classes / number of samples in the group\n",
    "\t\t\t\tp = class_sub.count(class_value) / size\n",
    "\t\t\t\tscore += p **2\n",
    "\t\t# Gini index = [1-∑ (proportion * proportion)] * (group size/ dataset size)\n",
    "\t\tgini += (1.0 - score) * (size / count_samples)\n",
    "\treturn gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tunique_class_val = list(set(row[-1] for row in dataset))\n",
    "\tb_score=9999\n",
    "    # get the number of columns  \n",
    "\tn_feature = len(dataset[0])-1 \n",
    "\tfor ref in range(n_feature):\n",
    "        # get unique values in the column  \n",
    "\t\tunique_value=set([row[ref] for row in dataset])\n",
    "\t\tfor criteria in unique_value:\n",
    "            #skip the class 0 and 9 as they cannot split the dataset into two groups\n",
    "\t\t\tif criteria==0 or criteria==9:\n",
    "\t\t\t\tcontinue       \n",
    "\t\t\tDataSplit = data_split(ref, criteria, dataset)\n",
    "\t\t\tgini = gini_index(DataSplit, unique_class_val)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_ref = ref \n",
    "\t\t\t\tb_value = criteria \n",
    "\t\t\t\tb_score = gini \n",
    "\t\t\t\tb_DataSplit = DataSplit\n",
    "\treturn {'ref':b_ref, 'value':b_value, 'branch':b_DataSplit}\n",
    " \n",
    "# Create a terminal node value\n",
    "def terminal(group):\n",
    "\tresult_class_val = [row[-1] for row in group]\n",
    "    #use most common class value as the output for terminal node\n",
    "\toutput=max(set(result_class_val), key=result_class_val.count)\n",
    "\treturn output\n",
    " \n",
    "# Create child-node and terminal node\n",
    "def split(node, max_depth, min_leaf_size, depth):\n",
    "\tl, r = node['branch']\n",
    "    #delete node as it is not needed\n",
    "\tdel(node['branch'])\n",
    "\tlen_l=len(l)\n",
    "\tlen_r=len(r)\t\n",
    "\t# if a split is a pure split, i.e. all value in a split belong to the same group. \n",
    "    # we wouldn’t be able to further split the data.\n",
    "\tif not l or not r:\n",
    "\t\tnode['l'] = node['r'] = terminal(l + r)\n",
    "\t\treturn\n",
    "\t# if it is over the max_depth, no further split required\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['l'], node['r'] = terminal(l), terminal(r)\n",
    "\t\treturn\n",
    "\t# if leave size less than min_leaf_size, no further split required\n",
    "\tif len_l <= min_leaf_size:      \n",
    "\t\tnode['l'] = terminal(l)\n",
    "\telse:\n",
    "        # if leave size is bigger than min_leaf_size, proceed with further split\n",
    "\t\tnode['l'] = get_split(l)\n",
    "\t\tdepth+=1\n",
    "\t\tsplit(node['l'], max_depth, min_leaf_size, depth)\n",
    "\t# if leave size less than min_leaf_size, no further split required\n",
    "\tif len_r <= min_leaf_size:\n",
    "\t\tnode['r'] = terminal(r)\n",
    "\telse:\n",
    "        # if leave size is bigger than min_leaf_size, proceed with further split\n",
    "\t\tnode['r'] = get_split(r)\n",
    "\t\tdepth+=1\n",
    "\t\tsplit(node['r'], max_depth, min_leaf_size, depth)\n",
    " \n",
    " # Call all functions needed to build the decision tree\n",
    "def build_tree(train, max_depth, min_leaf_size):\n",
    "\ttree = get_split(train)\n",
    "    # set initial depth as 1 as the split starting from root\n",
    "\tsplit(tree, max_depth, min_leaf_size, 1)\n",
    "\treturn tree\n",
    " \n",
    "# Predict with decision tree\n",
    "def predict(tree, row):\n",
    "    # tree has three components: ref, value, branch\n",
    "    # the below code compares data value to criteria for split and determine if the left or right branch should be followed\n",
    "\tif row[tree['ref']] < tree['value']:\n",
    "        # check if the a ternimal node is reached\n",
    "\t\tif isinstance(tree['l'], dict):\n",
    "\t\t\treturn predict(tree['l'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn tree['l']\n",
    "\telse:\n",
    "        # check if the a ternimal node is reached\n",
    "\t\tif isinstance(tree['r'], dict):\n",
    "\t\t\treturn predict(tree['r'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn tree['r']\n",
    "\n",
    " \n",
    " # combin everything together\n",
    "def decision_tree(train, test, max_depth, min_leaf_size):\n",
    "\ttree = build_tree(train, max_depth, min_leaf_size)\n",
    "    # set prediction result as list\n",
    "\tpredictions = list()\n",
    "    # preform the prediction row by row\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(tree, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = decision_tree(Train_data,Access_data,19,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6085\n"
     ]
    }
   ],
   "source": [
    "def accuracy(pred,y):\n",
    "    Correct_count=0\n",
    "    for i in range(0,len(pred)):\n",
    "        if pred[i]==y[i]:\n",
    "            Correct_count+=1 \n",
    "    return Correct_count/len(y)\n",
    "\n",
    "Pred_Accuracy = accuracy(pred,label_test)\n",
    "print(Pred_Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
