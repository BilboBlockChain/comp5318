{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with h5py.File('./data/train/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./data/train/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "\n",
    "# using H['datatest'], H['labeltest'] for test dataset.\n",
    "print(data_train.shape,label_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we seperate 5000 records from the train data set to use as a validation set. We verify that the last 5000 records of the training set are a representative sample of the 10 classes in Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 507, 1: 478, 2: 523, 3: 511, 4: 467, 5: 508, 6: 499, 7: 514, 8: 490, 9: 503}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(label_train[25000:,], return_counts=True)\n",
    "print(dict(zip(unique, counts))) #display counts of classes in candidate validation set\n",
    "\n",
    "#separate training and validation set \n",
    "data_val = data_train[25000:,]\n",
    "data_partial_train = data_train[:25000,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform the same train/val split on the labels training set. We then one hot encode the labels vector, changing its shape from size (samples,) to (samples, classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 784)\n"
     ]
    }
   ],
   "source": [
    "#one hot encode y for softmax output \n",
    "def oneHot(y):\n",
    "    zeroesY = np.zeros((y.size, y.max() + 1)) #generate matrix of zeroes shape (samples, classes)\n",
    "    zeroesY[np.arange(y.size), y] = 1 #insert value 1 at label y's scalar class value\n",
    "    return zeroesY\n",
    "\n",
    "label_val = oneHot(label_train[25000:,]) \n",
    "label_partial_train = oneHot(label_train[: 25000,])\n",
    "\n",
    "print(data_partial_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then reshape the train and val data tensors of shape (samples,28,28) to matrices of shape (samples, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 784)\n",
      "(25000, 784) (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(data_partial_train.shape)\n",
    "\n",
    "data_partial_train = data_partial_train.reshape(data_partial_train.shape[0], 28 * 28)\n",
    "data_val = data_val.reshape(data_val.shape[0], 28 * 28)\n",
    "\n",
    "print(data_partial_train.shape, data_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we define the SVD fitting function. This takes in two parameters (data matrix, number of components), performs SVD decomposition and returns the right singular value V of shape (m,k) where m is the dimension of features and k is the choice of number of leading components from the SVD decomposition. When k < m dimension reduction has been performed. \n",
    "We perform this fitting procedure on the training set only (taking 150 components) and use the same V for the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 150)\n",
      "(25000, 150) (5000, 150)\n"
     ]
    }
   ],
   "source": [
    "def svd_fit(A, comps):   \n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    #the matrix XV = UD therefore only need right singular value for decomposition\n",
    "    V_tilde = Vt.T[:,0:comps] #create parameter to pick number of leading components to take from V\n",
    "    return V_tilde\n",
    "\n",
    "v = svd_fit(flat_data_partial_train, 150)\n",
    "\n",
    "print(v.shape)\n",
    "\n",
    "#take dot product of train and val set with V to perform dimension reduction\n",
    "dim_partial_train = data_partial_train.dot(v)\n",
    "dim_data_val = data_val.dot(v)\n",
    "\n",
    "print( dim_partial_train.shape, dim_data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move onto the building blocks of the softmax classifer. <br>\n",
    "<br>\n",
    "First we define the softmax function. it takes in the matrix X.W which has shape (samples,classes) and produces a matrix of shape (samples, classes) that sums to one per sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X): \n",
    "    eX = np.exp(X)\n",
    "    A = eX / eX.sum(axis = 1, keepdims = True) #here we sum along classes per sample and broadcast this as the denominator\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the loss function taking in the paramaters (data matrix X, one hot encoded labels y, weight matrix W, regulariser penalty l): \n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{w}, l)=-\\sum_{n} \\sum_{k} y_{n k} \\log \\left(\\sigma_{\\boldsymbol{w}}(\\boldsymbol{x})\\right)+\\frac{l}{2}\\|\\boldsymbol{w}\\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "Where sigma denotes the softmax transformation. The sum over all samples and all classes is captured in np.sum() over the matrix with shape (samples,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define softmax loss \n",
    "def softmax_loss(X, Y, W, l):\n",
    "    A = softmax(X.dot(W))\n",
    "    n = len(X)\n",
    "    snorm = np.linalg.norm(W)**2\n",
    "    return (-1/n) * np.sum(Y * np.log(A)) + (l/2)*snorm #np.sum over the matrix captures summing over both classes and samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the direct gradient of the above loss function by creating a function that takes in the same paramaters:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{n}\\left(\\sigma_{w}\\left(\\boldsymbol{x}_{n}\\right)-y_{n}\\right) \\boldsymbol{x}^{T}+l \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "The output takes the shape (features, classes), preparing to be minused from the identical shaped weight vector W.\n",
    "We slightly change the above formulation to compute the gradient over the whole data matrix with shape (samples, features). This requires a dot product with the matrix (A-Y) that has shape (samples, classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define softmax gradient \n",
    "def softmax_grad(X, Y, W, l):\n",
    "    A = softmax(X.dot(W))  \n",
    "    n = len(X) \n",
    "    return  (np.dot(X.T,(A - Y))/n) + l*W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the softmax loss and gradients defined we can now define the fitting procedure. Here we employ batch gradient descent. This procedure involves choosing a randomly sampled without replacement batch size iteratively to perform gradient operations on. This value times a learning rate alpha is then minused from a weight vector. This procedure is then repeated over the dataset for a certain number of epochs. \n",
    "\n",
    "This function takes in a data matrix, the one hot encoded labels, a vector to optimise, a ridge penalty l, a number of epochs to optmise over and a number of batches per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_fit(X, Y, W, l, alpha, epoch,  batch):\n",
    "    n = len(X) #sample size\n",
    "    loss_hist = [softmax_loss(X, Y, W, l)] #generate intital loss history\n",
    "    steps = int(np.ceil(n/batch)) #define the number of steps per epoch, determined by how many batches fit in the sample\n",
    "    for ep in range(epoch): \n",
    "        p_ids = np.random.permutation(n) #generate a set of shuffled ids the size of the sample\n",
    "        shuffle_X = X[p_ids] #apply this shuffled id to data matrix\n",
    "        shuffle_Y = Y[p_ids] #apply this shuffled id to label matrix\n",
    "        for i in range(steps): \n",
    "            # get the i-th batch\n",
    "            X_batch = shuffle_X[i:i + batch, :] #subset data matrix to size batch starting at step point i\n",
    "            Y_batch = shuffle_Y[i:i + batch] #perform same operation to label matrix\n",
    "            W -=  alpha * softmax_grad(X_batch, Y_batch, W, l) #minus gradient for current w times learning rate from next w\n",
    "        loss_hist.append(softmax_loss(X, Y, W, l)) #persist loss over runs\n",
    "    return W, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the softmax fitting algorithm we now generate initial values for the weight vector with shape (features, classes) and run the fitting algorithm on our chosen hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_rand = np.random.randn(dim_partial_train.shape[1], len(label_partial_train[1]))\n",
    "W, loss_hist = softmax_fit(dim_partial_train, \n",
    "                                      label_partial_train , \n",
    "                                      W_rand, \n",
    "                                      epoch = 1500, \n",
    "                                      batch = 1000,\n",
    "                                      alpha = 0.1, \n",
    "                                      l = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict fuction\n",
    "def pred(W, X):\n",
    "    A = softmax(X.dot(W))\n",
    "    return np.argmax(A, axis = 1)\n",
    "\n",
    "#accuracy\n",
    "def accuracy(y_pred,y):\n",
    "    results = pd.DataFrame({'label': y, 'answer': y == y_pred})\n",
    "    return results['answer'].sum()/len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy of model on test set: 0.841\n"
     ]
    }
   ],
   "source": [
    "y_pre = pred(W,dim_data_val)\n",
    "print(\"Accurancy of model on test set:\",accuracy(y_pre,label_train[25000:,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/test/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./data/test/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(data_test.shape)\n",
    "\n",
    "#subset to labelled set\n",
    "data_test_label = data_test[:2000,:]\n",
    "\n",
    "data_test_label = data_test_label.reshape(data_test_label.shape[0], 28 * 28)\n",
    "\n",
    "dim_data_test = data_test.dot(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(W, dim_data_test )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
