{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 7 - Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2019**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To learn about using Gradient Decent\n",
    "* To learn about building a Logistic Regression model for multiple classes from Scratch\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab8.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab8.ipynb\" file\n",
    "* Complete exercises in \"lab7.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran\n",
    "\n",
    "Tutors: Fengxiang He, Shaojun Zhang, Fangzhou Shi, Yang Lin, Iwan Budiman, Zhiyi Wang, Canh Dinh, Yixuan Zhang, Rui Dong, Haoyu He, Dai Hoang Tran, Peibo Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Class Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Softmax function:\n",
    "Input: $\\textbf{x} \\in \\textbf{R}^{D}$\n",
    "\n",
    "Assume that there are $K$ different classes ${C_1,C_2 ..., C_k}$. For each class $C_k$, we have parameter vector $\\textbf{w}_k$ and model the posterior probability as:\n",
    "$$\n",
    "h_\\textbf{w}(\\textbf{x}) = p(y = C_k|\\textbf{x};\\textbf{w}) = \\frac{\\exp(\\textbf{w}_k^T\\textbf{x})}{\\sum_{k=1}^K \\exp(\\textbf{w}_k^T\\textbf{x})} \n",
    "%- y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j = \\Pr(c = 1|x) - y_j\n",
    "$$\n",
    "Decision boundary: Assign $\\textbf{x}$ with the label that is the maximum of posterior:\n",
    "$$\n",
    "argmax_k P(C_k|\\textbf{x};\\textbf{w}) \\rightarrow argmax_k \\textbf{w}_k^T\\textbf{x}\n",
    "$$\n",
    "\n",
    "The formula of multinomial logistics loss is:\n",
    "$$\n",
    "\\textbf{J}(\\textbf{w}) = - \\left[\\sum_{n=1}^{N} \\sum_{k=1}^K 1_{\\{y^{n} = C_k\\}}log(h_\\textbf{w}(\\textbf{x}^n))\\right] = - \\left[\\sum_{n=1}^{N} \\sum_{k=1}^K 1_{\\{y^{n} = C_k\\}}log(y^{n} =C_k| \\textbf{x}^{n};\\textbf{w})\\right]\n",
    "$$\n",
    "With $N$ is total number of sample\n",
    "\n",
    "**When K = 2, Multinomial model reduce to binary logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1:** Define Softmax function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z): \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gradient Descent for logistic regression\n",
    "With $\\eta $ is step size we have the gradient update:\n",
    "$$\n",
    "w_{t+1} \\leftarrow w_{t} - \\eta \\frac{1}{N}\\nabla  \\textbf{J}(\\textbf{w})\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} \\leftarrow w_{t} - \\eta \\left(\\frac{1}{N}\\sum_{n=1}^N(h_w(x^{n}) - y^{n})x^{n}\\right)\n",
    "$$\n",
    "\n",
    "Adding regularization term :\n",
    "$$w_{t+1} \\leftarrow w_{t} - \\eta \\left(\\frac{1}{N}\\sum_{n=1}^N(h_w(x^{n}) - y^{n})x^{n} +\\lambda w_t \\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient \n",
    "def softmax_grad(X, y, W):\n",
    "    A = softmax(X.dot(W))    # shape of (N, C)\n",
    "    id0 = range(X.shape[0])  # number of train data\n",
    "    A[id0, y] -= 1           # A - Y, shape of (N, C)\n",
    "    return X.T.dot(A)/X.shape[0]\n",
    "    '''\n",
    "    onehot = np.zeros((len(y), C ))\n",
    "    onehot[np.arange(len(y)), y] = 1\n",
    "    A = softmax_stable(x.dot(W))\n",
    "    A = A - onehot\n",
    "    x.T.dot(A2)/x.shape[0]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define learning function for multi-logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost or loss function  \n",
    "def softmax_loss(X, y, W):\n",
    "    A = softmax(X.dot(W))\n",
    "    id0 = range(X.shape[0])\n",
    "    return -np.mean(np.log(A[id0, y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2:** Calculate W after each update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building learning function using softmax gradient descent\n",
    "def softmax_fit(X, y, W, lr = 0.01, nepoches = 100, tol = 1e-5, batch_size = 10):\n",
    "    W_old = W.copy()\n",
    "    ep = 0 \n",
    "    loss_hist = [softmax_loss(X, y, W)] # store history of loss \n",
    "    N = X.shape[0]\n",
    "    nbatches = int(np.ceil(float(N)/batch_size))\n",
    "    while ep < nepoches: \n",
    "        ep += 1 \n",
    "        mix_ids = np.random.permutation(N) # mix data \n",
    "        for i in range(nbatches):\n",
    "            # get the i-th batch\n",
    "            batch_ids = mix_ids[batch_size*i:min(batch_size*(i+1), N)] \n",
    "            X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "            W -=  # TODO\n",
    "        loss_hist.append(softmax_loss(X, y, W))\n",
    "        if np.linalg.norm(W - W_old)/W.size < tol:\n",
    "            break \n",
    "        W_old = W.copy()\n",
    "    return W, loss_hist \n",
    "\n",
    "# predict function\n",
    "def pred(W, X):\n",
    "    A = softmax(X.dot(W))\n",
    "    return np.argmax(A, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying the algorithm with dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate dataset with 5 different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 5    # number of classes\n",
    "N = 800  # number of points per class \n",
    "means = [[2, 2], [8, 3], [3, 6], [14, 2], [12, 8]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "X3 = np.random.multivariate_normal(means[3], cov, N)\n",
    "X4 = np.random.multivariate_normal(means[4], cov, N)\n",
    "\n",
    "X = np.concatenate((X0, X1, X2, X3, X4), axis = 0) # each row is a datapoint\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis = 1) # bias trick \n",
    "y = np.asarray([0]*N + [1]*N + [2]*N+ [3]*N + [4]*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(X, label):\n",
    "    X0 = X[np.where(label == 0)[0]]\n",
    "    X1 = X[np.where(label == 1)[0]]\n",
    "    X2 = X[np.where(label == 2)[0]]\n",
    "    X3 = X[np.where(label == 3)[0]]\n",
    "    X4 = X[np.where(label == 4)[0]]\n",
    "    plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\n",
    "    plt.plot(X1[:, 0], X1[:, 1], 'co', markersize = 4, alpha = .8)\n",
    "    plt.plot(X2[:, 0], X2[:, 1], 'gs', markersize = 4, alpha = .8)\n",
    "    plt.plot(X3[:, 0], X3[:, 1], 'y.', markersize = 4, alpha = .8)\n",
    "    plt.plot(X4[:, 0], X4[:, 1], 'r*', markersize = 4, alpha = .8)\n",
    "    plt.plot()\n",
    "    \n",
    "display(X,y)\n",
    "plt.title(\"5 different classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Separate the dataset into training set and test set to train Multi-logistic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xbar, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_init = np.random.randn(X_train.shape[1], C)\n",
    "W, loss_hist = softmax_fit(X_train, y_train, W_init, batch_size = 10, nepoches = 100, lr = 0.05)\n",
    "    \n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('number of epoches', fontsize = 13)\n",
    "plt.ylabel('loss', fontsize = 13)\n",
    "plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3:** Define the function to calculate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pre,y):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = pred(W,X_test)\n",
    "print(\"Accuracy of model on test set:\",accuracy(y_pre,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training model on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2, loss_hist2 = softmax_fit(Xbar, y, W_init, batch_size = 10, nepoches = 100, lr = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the final pattern for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = np.arange(-2, 18, 0.025)\n",
    "xlen = len(xm)\n",
    "ym = np.arange(-3, 11, 0.025)\n",
    "ylen = len(ym)\n",
    "xx, yy = np.meshgrid(xm, ym)\n",
    "xx1 = xx.ravel().reshape(-1, 1)\n",
    "yy1 = yy.ravel().reshape(-1, 1)\n",
    "XX = np.concatenate(( xx1, yy1, np.ones(( xx.size, 1))), axis = 1)\n",
    "\n",
    "Z = pred(W2, XX)\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS = plt.contourf(xx, yy, Z, 200, cmap='jet', alpha = .1)\n",
    "plt.xlim(-2, 18)\n",
    "plt.ylim(-3, 11)\n",
    "plt.title(\"Final pattern for each class\")\n",
    "display(X, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
